# dt_pipeline_airflow_spark
Creating a data pipelines extracting data from Twitter, using apache airflow, passing to Data Lake. Then, use spark for extract raw data from data lake, transforme them and loading in data lake again.
